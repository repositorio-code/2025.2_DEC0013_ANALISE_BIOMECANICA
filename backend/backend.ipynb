{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONMF79YOhe27"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# PARTE 1: INSTALAÇÃO DAS DEPENDÊNCIAS\n",
        "# ===============================================================\n",
        "\n",
        "!rm -rf monocular-demos\n",
        "!pip install git+https://github.com/peabody124/GaitTransformer\n",
        "!git clone https://github.com/IntelligentSensingAndRehabilitation/monocular-demos.git\n",
        "!pip install -q fastapi uvicorn pyngrok python-multipart nest-asyncio\n",
        "\n",
        "%cd monocular-demos\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "# limit jax and TF from consuming all GPU memory\n",
        "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "\n",
        "#from tqdm.notebook import tqdm\n",
        "\n",
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from jaxtyping import Integer, Float, Array, PRNGKeyArray\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import equinox as eqx\n",
        "import optax\n",
        "\n",
        "import monocular_demos\n",
        "from monocular_demos.biomechanics_mjx.forward_kinematics import ForwardKinematics\n",
        "from monocular_demos.biomechanics_mjx.monocular_trajectory import KineticsWrapper, get_default_wrapper\n",
        "from monocular_demos.biomechanics_mjx.visualize import render_trajectory, jupyter_embed_video\n",
        "\n",
        "\n",
        "from gait_transformer.gait_phase_transformer import load_default_model, gait_phase_stride_inference\n",
        "from gait_transformer.visualization import make_overlay, jupyter_embed_video\n",
        "from gait_transformer.gait_phase_kalman import gait_kalman_smoother, compute_phases, get_event_times\n",
        "\n",
        "# Check if GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"TensorFlow is using the GPU\")\n",
        "else:\n",
        "    print(\"TensorFlow is not using the GPU\")\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# Check for available GPU devices\n",
        "num_devices = jax.local_device_count()\n",
        "print(f\"Found {num_devices} JAX devices:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwmmqA6eOQUJ"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# PARTE 2: CÓDIGO DE PROCESSAMENTO ADAPTADO\n",
        "# ===============================================================\n",
        "def processador_de_video(video_filepath: str, output_dir: str, joint_selection: str = \"Joelho\", height_mm: int = 1778):\n",
        "  \"\"\"\n",
        "  Executa o pipeline completo de análise biomecânica do notebook.\n",
        "  Retorna um dicionário com os caminhos para todos os arquivos de saída gerados.\n",
        "  \"\"\"\n",
        "  fk = ForwardKinematics()\n",
        "\n",
        "  ### ALTERAÇÃO: Configuração do diretório de saída e nomes de arquivo ###\n",
        "  base_filename = os.path.splitext(os.path.basename(video_filepath))[0]\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  results = {}\n",
        "  print(f\"Iniciando análise para o vídeo: {video_filepath}\")\n",
        "  print(f\"Os resultados serão salvos em: {output_dir}\")\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "  cap = cv2.VideoCapture(video_filepath)\n",
        "  ret, frame = cap.read()\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  ### ALTERAÇÃO: Salva o Gráfico 1 (Frame Inicial) ###\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.imshow(frame)\n",
        "  plt.title(f'Frame Inicial de {os.path.basename(video_filepath)}')\n",
        "  plt.axis('off')\n",
        "  path_grafico_1 = os.path.join(output_dir, f\"{base_filename}_01_frame_inicial.png\")\n",
        "  plt.savefig(path_grafico_1)\n",
        "  plt.close()\n",
        "  results['grafico_frame_inicial'] = path_grafico_1\n",
        "\n",
        "  # Se o frame aparecer transposto, defina isto como True ---------------------\n",
        "  # (Nota: a sobreposição do gait transformer não ficará correta)\n",
        "  rotated = False\n",
        "\n",
        "  # Estimação de Keypoints (MeTAbs com skeleton 'bml_movi_87') ---------------------------\n",
        "  model = hub.load('https://bit.ly/metrabs_l')  # Takes about 3 minutes\n",
        "  skeleton = 'bml_movi_87'\n",
        "  joint_names = model.per_skeleton_joint_names[skeleton].numpy().astype(str)\n",
        "  joint_edges = model.per_skeleton_joint_edges[skeleton].numpy()\n",
        "\n",
        "  from monocular_demos.utils import joint_names, video_reader\n",
        "\n",
        "  # Lê o vídeo em lotes ------------------------------------------------\n",
        "  vid, n_frames = video_reader(video_filepath)\n",
        "\n",
        "  print(f'About to processs {video_filepath} which has {n_frames} frames')\n",
        "  accumulated = None\n",
        "  for i, frame_batch in tqdm(enumerate(vid), total=n_frames//8):\n",
        "      if rotated:\n",
        "          frame_batch = frame_batch.transpose(0, 2, 1, 3)\n",
        "\n",
        "      pred = model.detect_poses_batched(frame_batch, skeleton=skeleton)\n",
        "\n",
        "      if accumulated is None:\n",
        "          accumulated = pred\n",
        "\n",
        "      else:\n",
        "          for key in accumulated.keys():\n",
        "              accumulated[key] = tf.concat([accumulated[key], pred[key]], axis=0)\n",
        "\n",
        "      # if i > 10:\n",
        "      #     break\n",
        "\n",
        "  # Verifica o numero de pessoas detectadas por frame ------------------------------------------------\n",
        "  num_people = [p.shape[0] for p in accumulated['poses2d']]\n",
        "\n",
        "  if 0 in set(num_people):\n",
        "      print('**WARNING** some frames with no people, make fail')\n",
        "\n",
        "  # assert this is 1 for all the frames\n",
        "  #assert len(set(num_people)) == 1\n",
        "\n",
        "  boxes = np.array([p[0] for p in accumulated['boxes'] if len(p) > 0])\n",
        "  pose3d = np.array([p[0] for p in accumulated['poses3d'] if len(p) > 0])\n",
        "  pose2d = np.array([p[0] for p in accumulated['poses2d'] if len(p) > 0])\n",
        "\n",
        "  # Para conveniência, salve os keypoints caso o notebook trave ou precise reiniciar\n",
        "  with open('keypoints3d.npz', 'wb') as f:\n",
        "      np.savez(f, pose3d)\n",
        "\n",
        "\n",
        "  frame_idx = 0\n",
        "\n",
        "  # Seleciona apenas os nomes das articulações que possuem letras maiúsculas\n",
        "  capitalized_joint_names = [name for name in joint_names if any(c.isupper() for c in name)]\n",
        "\n",
        "  # Encontra os índices dessas articulações na lista original\n",
        "  capitalized_joint_indices = [joint_names.index(name) for name in capitalized_joint_names]\n",
        "\n",
        "  # Extrai os keypoints do frame selecionado apenas para as articulações com letras maiúsculas\n",
        "  keypoints_t0_capitalized = pose3d[frame_idx, capitalized_joint_indices, :]\n",
        "\n",
        "  # Defina as conexões (arestas) para as articulações com letras maiúsculas.\n",
        "  # Você deve definir manualmente essas conexões com base nas ligações anatômicas\n",
        "  # que deseja visualizar entre as articulações selecionadas.\n",
        "  # Esta é uma etapa crucial, pois o array original joint_edges pode não mapear\n",
        "  # diretamente para o subconjunto de articulações com letras maiúsculas.\n",
        "  # Como exemplo, vamos definir algumas conexões potenciais com base na anatomia humana comum.\n",
        "  # Você precisará ajustar isso com base nas conexões reais que deseja exibir.\n",
        "  # Os índices aqui se referem aos índices *dentro* da lista `capitalized_joint_names`.\n",
        "  capitalized_joint_edges = [\n",
        "      (capitalized_joint_names.index('CHip'), capitalized_joint_names.index('LHip')),\n",
        "      (capitalized_joint_names.index('CHip'), capitalized_joint_names.index('RHip')),\n",
        "      (capitalized_joint_names.index('LHip'), capitalized_joint_names.index('LKnee')),\n",
        "      (capitalized_joint_names.index('RHip'), capitalized_joint_names.index('RKnee')),\n",
        "      (capitalized_joint_names.index('LKnee'), capitalized_joint_names.index('LAnkle')),\n",
        "      (capitalized_joint_names.index('RKnee'), capitalized_joint_names.index('RAnkle')),\n",
        "      (capitalized_joint_names.index('LAnkle'), capitalized_joint_names.index('LFoot')),\n",
        "      (capitalized_joint_names.index('RAnkle'), capitalized_joint_names.index('RFoot')),\n",
        "      (capitalized_joint_names.index('CHip'), capitalized_joint_names.index('Neck')),\n",
        "      (capitalized_joint_names.index('Neck'), capitalized_joint_names.index('Head')),\n",
        "      (capitalized_joint_names.index('Neck'), capitalized_joint_names.index('LShoulder')),\n",
        "      (capitalized_joint_names.index('Neck'), capitalized_joint_names.index('RShoulder')),\n",
        "      (capitalized_joint_names.index('LShoulder'), capitalized_joint_names.index('LElbow')),\n",
        "      (capitalized_joint_names.index('RShoulder'), capitalized_joint_names.index('RElbow')),\n",
        "      (capitalized_joint_names.index('LElbow'), capitalized_joint_names.index('LWrist')),\n",
        "      (capitalized_joint_names.index('RElbow'), capitalized_joint_names.index('RWrist')),\n",
        "      (capitalized_joint_names.index('LWrist'), capitalized_joint_names.index('LHand')),\n",
        "      (capitalized_joint_names.index('RWrist'), capitalized_joint_names.index('RHand')),\n",
        "  ]\n",
        "\n",
        "  # Create a 3D scatter plot------------------------------------------------------------\n",
        "  fig = plt.figure(figsize=(10, 8))\n",
        "  fig.suptitle('Visualização do Esqueleto no Frame 0')\n",
        "  ax = fig.add_subplot(121, projection='3d')\n",
        "\n",
        "  # Scatter plot of the capitalized keypoints\n",
        "  ax.scatter(keypoints_t0_capitalized[:, 0], keypoints_t0_capitalized[:, 2], keypoints_t0_capitalized[:, 1])\n",
        "\n",
        "  # Add labels for each capitalized joint\n",
        "  for i, (x, y, z) in enumerate(keypoints_t0_capitalized):\n",
        "      ax.text(x, y, z, capitalized_joint_names[i], fontsize=9)\n",
        "\n",
        "  # Draw lines connecting the capitalized joints based on the defined edges\n",
        "  for i, j in capitalized_joint_edges:\n",
        "      ax.plot(\n",
        "          [keypoints_t0_capitalized[i, 0], keypoints_t0_capitalized[j, 0]],\n",
        "          [keypoints_t0_capitalized[i, 2], keypoints_t0_capitalized[j, 2]],\n",
        "          [keypoints_t0_capitalized[i, 1], keypoints_t0_capitalized[j, 1]],\n",
        "          'k-',  # 'k-' means black solid line\n",
        "          linewidth=1\n",
        "      )\n",
        "\n",
        "\n",
        "  # Set labels for the axes\n",
        "  ax.set_xlabel('X')\n",
        "  ax.set_ylabel('Y (element 2)')\n",
        "  ax.set_zlabel('Z (element 1)')\n",
        "\n",
        "  # Set a title for the plot\n",
        "  ax.set_title('3D Scatter Plot and Skeleton')\n",
        "\n",
        "  # Set equal aspect ratio\n",
        "  max_range = np.array([keypoints_t0_capitalized[:,0].max() - keypoints_t0_capitalized[:,0].min(),\n",
        "                        keypoints_t0_capitalized[:,2].max() - keypoints_t0_capitalized[:,2].min(),\n",
        "                        keypoints_t0_capitalized[:,1].max() - keypoints_t0_capitalized[:,1].min()]).max()\n",
        "\n",
        "  mid_x = (keypoints_t0_capitalized[:,0].max() + keypoints_t0_capitalized[:,0].min()) * 0.5\n",
        "  mid_y = (keypoints_t0_capitalized[:,2].max() + keypoints_t0_capitalized[:,2].min()) * 0.5\n",
        "  mid_z = (keypoints_t0_capitalized[:,1].max() + keypoints_t0_capitalized[:,1].min()) * 0.5\n",
        "\n",
        "  ax.set_xlim(mid_x - max_range * 0.5, mid_x + max_range * 0.5)\n",
        "  ax.set_ylim(mid_y - max_range * 0.5, mid_y + max_range * 0.5)\n",
        "  ax.set_zlim(mid_z + max_range * 0.5, mid_z - max_range * 0.5)\n",
        "\n",
        "\n",
        "\n",
        "  ax = fig.add_subplot(122)\n",
        "\n",
        "  pose = pose3d[frame_idx]\n",
        "  # pose /= 1000.0\n",
        "  pose = pose - np.mean(pose, axis=0)\n",
        "\n",
        "  pose = pose[:, [0, 2, 1]]\n",
        "  pose[:, 2] *= -1\n",
        "\n",
        "  ax.plot(pose[:, 0], pose[:, 2], '.')\n",
        "  for e in joint_edges:\n",
        "      ax.plot(pose[e, 0], pose[e, 2], 'k')\n",
        "  for i, p in enumerate(pose):\n",
        "      ax.text(p[0]+0.05, p[2], f'{i}: {joint_names[i]}', fontsize=8)\n",
        "\n",
        "  ax.axis('equal')\n",
        "\n",
        "  # Show the plot\n",
        "  path_grafico_2 = os.path.join(output_dir, f\"{base_filename}_02_visualizacao_esqueleto.png\")\n",
        "  plt.savefig(path_grafico_2)\n",
        "  #plt.show()\n",
        "  plt.close(fig)\n",
        "  results['grafico_esqueleto_3d_2d'] = path_grafico_2\n",
        "\n",
        "  #---------------------------------------------------------------------\n",
        "\n",
        "  with open('keypoints3d.npz', 'rb') as f:\n",
        "    pose3d = np.load(f, allow_pickle=True)['arr_0']\n",
        "\n",
        "  # exclude any frames where people were missed to make the code work reliably\n",
        "  pose3d = np.array([p[0] for p in accumulated['poses3d']])\n",
        "\n",
        "  # build a dataset that includes the timestamps and 3D pose estimates\n",
        "\n",
        "  #---------------------------------------------------------------------\n",
        "\n",
        "  # convert pose to m\n",
        "  pose = pose3d\n",
        "  pose = pose[:, :, [0, 2, 1]]\n",
        "  pose[:, :, 2] *= -1\n",
        "  pose /= 1000.0\n",
        "\n",
        "  pose = pose - np.min(pose, axis=1, keepdims=True)\n",
        "\n",
        "  timestamps = jnp.arange(len(pose)) / 30.0\n",
        "\n",
        "  dataset = (timestamps, pose)\n",
        "\n",
        "  #-----------------------------------------------------------------\n",
        "\n",
        "  # construct a loss function between the forward pass through the forward kinematic\n",
        "  # implicit representation and the resulting keypoint and the detected keypoitns\n",
        "\n",
        "  def loss(\n",
        "      model: KineticsWrapper,\n",
        "      x: Float[Array, \"times\"],\n",
        "      y: Float[Array, \"times keypoints 3\"],\n",
        "      site_offset_regularization = 1e-1\n",
        "  ) -> Tuple[Float, Dict]:\n",
        "\n",
        "      timestamps = x\n",
        "      keypoints3d = y\n",
        "      metrics = {}\n",
        "\n",
        "      # NOTE: steps is an make sure this retraces for different dimensions\n",
        "      (state, constraints, next_states), (ang, vel, action), _ = model(\n",
        "          timestamps,\n",
        "          skip_vel=True,\n",
        "          skip_action=True,\n",
        "      )\n",
        "\n",
        "      pred_kp3d = state.site_xpos\n",
        "\n",
        "      l = jnp.mean((pred_kp3d - keypoints3d) ** 2) * 100 # so in cm\n",
        "      metrics[\"kp_err\"] = l\n",
        "\n",
        "      # regularize marker offset\n",
        "      l_site_offset = jnp.sum(jnp.square(model.site_offsets))\n",
        "      l += l_site_offset * site_offset_regularization\n",
        "\n",
        "      # make loss the first key in the dictionary by popping and building a new dictionary with the rest\n",
        "      metrics = {\"loss\": l, **metrics}\n",
        "\n",
        "      return l, metrics\n",
        "\n",
        "\n",
        "  @eqx.filter_jit\n",
        "  def step(model, opt_state, data, loss_grad, optimizer, **kwargs):\n",
        "      x, targets = data\n",
        "\n",
        "      (val, metrics), grads = loss_grad(model, x=x, y=targets, **kwargs)\n",
        "      updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "      model = eqx.apply_updates(model, updates)\n",
        "      return val, model, opt_state, metrics\n",
        "\n",
        "\n",
        "  def fit_model(\n",
        "      model: KineticsWrapper,\n",
        "      dataset: Tuple,\n",
        "      lr_end_value: float = 1e-8,\n",
        "      lr_init_value: float = 1e-4,\n",
        "      max_iters: int = 5000,\n",
        "      clip_by_global_norm: float = 0.1,\n",
        "  ):\n",
        "\n",
        "      # work out the transition steps to make the desired schedule\n",
        "      transition_steps = 10\n",
        "      lr_decay_rate = (lr_end_value / lr_init_value) ** (1.0 / (max_iters // transition_steps))\n",
        "      learning_rate = optax.warmup_exponential_decay_schedule(\n",
        "          init_value=0,\n",
        "          warmup_steps=0,\n",
        "          peak_value=lr_init_value,\n",
        "          end_value=lr_end_value,\n",
        "          decay_rate=lr_decay_rate,\n",
        "          transition_steps=transition_steps,\n",
        "      )\n",
        "\n",
        "      optimizer = optax.chain(\n",
        "          optax.adamw(learning_rate=learning_rate, b1=0.8, weight_decay=1e-5), optax.zero_nans(), optax.clip_by_global_norm(clip_by_global_norm)\n",
        "      )\n",
        "      opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
        "\n",
        "      loss_grad = eqx.filter_value_and_grad(loss, has_aux=True)\n",
        "\n",
        "      counter = trange(max_iters)\n",
        "      for i in counter:\n",
        "\n",
        "          val, model, opt_state, metrics = step(model, opt_state, dataset, loss_grad, optimizer)\n",
        "\n",
        "          if i > 0 and i % int(max_iters // 10) == 0:\n",
        "              print(f\"\\niter: {i} loss: {val}.\")  # metrics: {metrics}\")\n",
        "\n",
        "          if i % 50 == 0:\n",
        "              metrics = {k: v.item() for k,v in metrics.items()}\n",
        "              counter.set_postfix(metrics)\n",
        "\n",
        "      return model, metrics\n",
        "\n",
        "\n",
        "  fkw = get_default_wrapper()\n",
        "  updated_model, metrics = fit_model(fkw, dataset)\n",
        "\n",
        "  #----------------------------------------------------------------\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # ALTERAÇÃO AQUI: Lógica de Seleção Dinâmica das Articulações\n",
        "  # ----------------------------------------------------------------\n",
        "\n",
        "  # Dicionário mapeando a escolha da interface para os nomes internos do modelo FK\n",
        "  # NOTA: Verifique se os nomes ('hip_flexion_r', etc) batem com seu fk.joint_names\n",
        "  # Dicionário atualizado com os nomes EXATOS do seu modelo\n",
        "  mapa_articulacoes = {\n",
        "      # Membros Inferiores\n",
        "      \"Joelho\":    {'nomes': ['knee_angle_r', 'knee_angle_l'],   'titulo': 'Flexão do Joelho'},\n",
        "      \"Quadril\":   {'nomes': ['hip_flexion_r', 'hip_flexion_l'], 'titulo': 'Flexão do Quadril'},\n",
        "      \"Tornozelo\": {'nomes': ['ankle_angle_r', 'ankle_angle_l'], 'titulo': 'Angulação do Tornozelo'},\n",
        "\n",
        "      # Membros Superiores\n",
        "      \"Ombro\":     {'nomes': ['arm_flex_r', 'arm_flex_l'],       'titulo': 'Flexão do Ombro'},\n",
        "      \"Cotovelo\":  {'nomes': ['elbow_flex_r', 'elbow_flex_l'],   'titulo': 'Flexão do Cotovelo'},\n",
        "      \"Punho\":     {'nomes': ['wrist_flex_r', 'wrist_flex_l'],   'titulo': 'Flexão do Punho'},\n",
        "  }\n",
        "\n",
        "  # Fallback para Joelho se não encontrar\n",
        "  selecao = mapa_articulacoes.get(joint_selection, mapa_articulacoes[\"Joelho\"])\n",
        "  target_joints = selecao['nomes']\n",
        "  plot_title = selecao['titulo']\n",
        "\n",
        "  # pass the timestamps through the fitted model to get the kinematics\n",
        "  (state, constraints, next_states), (ang, vel, action), _ = updated_model(dataset[0], skip_vel=True, skip_action=True)\n",
        "\n",
        "  # Busca dinâmica dos índices\n",
        "  try:\n",
        "      joint_idxs = jnp.array([fk.joint_names.index(n) for n in target_joints])\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(10, 5))\n",
        "      # Plota os dados (assumindo que são pares Direito/Esquerdo)\n",
        "      ax.plot(dataset[0], -np.degrees(ang[:, joint_idxs]))\n",
        "      ax.set_xlabel('Tempo (s)')\n",
        "      ax.set_ylabel('Ângulo (graus)')\n",
        "      ax.set_title(f'{plot_title} ao Longo do Tempo')\n",
        "      ax.legend(['Direito', 'Esquerdo']) # Ajuste conforme a ordem da lista\n",
        "\n",
        "      path_grafico_3 = os.path.join(output_dir, f\"{base_filename}_03_angulo_{joint_selection.lower()}.png\")\n",
        "      plt.savefig(path_grafico_3)\n",
        "      plt.close(fig)\n",
        "      results['grafico_angulos'] = path_grafico_3\n",
        "\n",
        "  except ValueError as e:\n",
        "      print(f\"Erro ao encontrar articulações: {e}\")\n",
        "      # Lógica de erro ou fallback aqui\n",
        "\n",
        "  '''  # pass the timestamps through the fitted model to get the kinematics\n",
        "  (state, constraints, next_states), (ang, vel, action), _ = updated_model(dataset[0], skip_vel=True, skip_action=True)\n",
        "\n",
        "  knee_idx = jnp.array([fk.joint_names.index(n) for n in ['knee_angle_r', 'knee_angle_l']])\n",
        "  # plot the knees\n",
        "  #plt.figure()\n",
        "  fig, ax = plt.subplots(figsize=(10, 5))\n",
        "  ax.plot(dataset[0], -np.degrees(ang[:, knee_idx]))\n",
        "  ax.set_xlabel('Tempo (s)')\n",
        "  ax.set_ylabel('Ângulo do Joelho (graus)')\n",
        "  ax.set_title('Flexão/Extensão do Joelho ao Longo do Tempo')\n",
        "  ax.legend(['Joelho Direito', 'Joelho Esquerdo'])\n",
        "  path_grafico_3 = os.path.join(output_dir, f\"{base_filename}_03_angulo_joelhos.png\")\n",
        "  plt.savefig(path_grafico_3)\n",
        "  plt.close(fig)\n",
        "  results['grafico_angulo_joelhos'] = path_grafico_3'''\n",
        "\n",
        "  #---------------------------------------------------------------------\n",
        "\n",
        "  # And create a MuJoCo visualization\n",
        "\n",
        "  fn = os.path.join(output_dir, f\"{base_filename}_reconstrucao.mp4\")\n",
        "  render_trajectory(ang, fn, xml_path=None)\n",
        "  results['video_reconstrucao'] = fn\n",
        "  HTML = jupyter_embed_video(fn)\n",
        "  HTML\n",
        "\n",
        "\n",
        "  #----------------------------------------------------------\n",
        "\n",
        "  # there are many skeleton formats support by this model. we are selecting one\n",
        "  # compatible with the gait transformer we will use below\n",
        "  skeleton = 'mpi_inf_3dhp_17'\n",
        "\n",
        "  # get the joint names and the edges between them for visualization below\n",
        "  joint_names = model.per_skeleton_joint_names[skeleton].numpy().astype(str)\n",
        "  joint_edges = model.per_skeleton_joint_edges[skeleton].numpy()\n",
        "\n",
        "\n",
        "  vid, n_frames = video_reader(video_filepath)\n",
        "\n",
        "  print(f'About to processs {video_filepath} which has {n_frames} frames')\n",
        "  accumulated = None\n",
        "  for i, frame_batch in tqdm(enumerate(vid), total=n_frames//8):\n",
        "      # use this for portrait videos on cell phone that are not detected\n",
        "      if rotated:\n",
        "          frame_batch = frame_batch.transpose(0, 2, 1, 3)\n",
        "\n",
        "      pred = model.detect_poses_batched(frame_batch, skeleton=skeleton)\n",
        "\n",
        "      if accumulated is None:\n",
        "          accumulated = pred\n",
        "\n",
        "      else:\n",
        "          # concatenate the ragged tensor along the batch for each element in the dictionary\n",
        "          for key in accumulated.keys():\n",
        "              accumulated[key] = tf.concat([accumulated[key], pred[key]], axis=0)\n",
        "\n",
        "  #----------------------------------------------------------------------------------------\n",
        "\n",
        "  num_people = [p.shape[0] for p in accumulated['poses2d']]\n",
        "\n",
        "  if 0 in set(num_people):\n",
        "      print('**WARNING** some frames with no people, make fail')\n",
        "\n",
        "  # assert this is 1 for all the frames\n",
        "  #assert len(set(num_people)) == 1\n",
        "\n",
        "  # then extract the information for that person\n",
        "  boxes = np.array([p[0] for p in accumulated['boxes'] if len(p) > 0])\n",
        "  pose3d = np.array([p[0] for p in accumulated['poses3d'] if len(p) > 0])\n",
        "  pose2d = np.array([p[0] for p in accumulated['poses2d'] if len(p) > 0])\n",
        "\n",
        "  #----------------------------------------------------------------------------------------\n",
        "\n",
        "  # this is the order of joints from the Gast-NET algorithm that the gait transformer was originally trained on\n",
        "  expected_order = ['pelv', 'rhip', 'rkne', 'rank', 'lhip', 'lkne', 'lank', 'spin', 'neck', 'head', 'htop', 'lsho', 'lelb', 'lwri', 'rsho', 'relb', 'rwri']\n",
        "  expected_order_idx = np.array([joint_names.tolist().index(j) for j in expected_order])\n",
        "\n",
        "  # GastNet also produces centered data\n",
        "  #keypoints = pose3d - np.mean(pose3d, axis=1, keepdims=True)\n",
        "  keypoints = pose3d - pose3d[:, joint_names.tolist().index('pelv'), None]\n",
        "\n",
        "  # we should also convert the mm output from MeTRAbs to the expected meters\n",
        "  keypoints = keypoints / 1000.0      # convert mm to m\n",
        "\n",
        "  # finally convert the axis order and signs to be compatible\n",
        "  keypoints = keypoints[:, :, [0, 2, 1]]\n",
        "  keypoints = keypoints[:, expected_order_idx]\n",
        "  keypoints[:, :, 2] *= -1\n",
        "\n",
        "  #----------------------------------------------------------------------------------------\n",
        "\n",
        "  transformer_model = load_default_model()\n",
        "\n",
        "  #------------------------------------------------------------------------------------------\n",
        "\n",
        "  # include the height of the participant\n",
        "  # nominally this should be correct but won't affect timing\n",
        "\n",
        "  height_mm = 1778\n",
        "\n",
        "  # set the window length the transformer processes. this should be enough\n",
        "  # to get at least a gait cycle or two\n",
        "  L = 90\n",
        "\n",
        "  phase, stride = gait_phase_stride_inference(keypoints, height_mm, transformer_model, L)\n",
        "\n",
        "  #---------------------------------------------------------------------------------------------\n",
        "\n",
        "  ### ALTERAÇÃO: Salva o Gráfico 4 (Fase da Marcha) ###\n",
        "  fig, ax = plt.subplots(figsize=(10, 5))\n",
        "  ax.plot(timestamps, phase[:, :4])\n",
        "  ax.set_title('Componentes da Fase da Marcha')\n",
        "  ax.set_xlabel('Tempo (s)')\n",
        "  ax.set_ylabel('Valor do Componente')\n",
        "  ax.legend(['cos(1)', 'cos(2)', 'cos(3)', 'cos(4)'])\n",
        "  path_grafico_4 = os.path.join(output_dir, f\"{base_filename}_04_fase_marcha.png\")\n",
        "  plt.savefig(path_grafico_4)\n",
        "  plt.close(fig)\n",
        "  results['grafico_fase_marcha'] = path_grafico_4\n",
        "\n",
        "  #----------------------------------------------------------------------------------------------\n",
        "\n",
        "  ### ALTERAÇÃO: Salva o Vídeo 2 (Overlay) ###\n",
        "  video_overlay_path = os.path.join(output_dir, f\"{base_filename}_overlay.mp4\")\n",
        "  phase_ordered = np.take(phase, [0, 4, 1, 5, 2, 6, 3, 7], axis=-1)\n",
        "  make_overlay(video_filepath, phase_ordered, stride, pose2d, video_overlay_path)\n",
        "  results['video_overlay'] = video_overlay_path\n",
        "\n",
        "  #-------------------------------------------------------------------------------\n",
        "\n",
        "  # kalman filter expects cos, sin alternating instead of the output from the gait transformer\n",
        "  # which is the four cos and then the four sin\n",
        "\n",
        "  phase_ordered = np.take(phase, [0, 4, 1, 5, 2, 6, 3, 7], axis=-1)\n",
        "  state, predictions, errors = gait_kalman_smoother(phase_ordered)\n",
        "\n",
        "  ### ALTERAÇÃO: Salva o Gráfico 5 (Erro do Filtro de Kalman) ###\n",
        "  fig, ax = plt.subplots(figsize=(10, 5))\n",
        "  ax.plot(timestamps, errors)\n",
        "  ax.set_title('Erro de Reconstrução do Filtro de Kalman')\n",
        "  ax.set_xlabel('Tempo (s)')\n",
        "  ax.set_ylabel('Erro')\n",
        "  path_grafico_5 = os.path.join(output_dir, f\"{base_filename}_05_erro_kalman.png\")\n",
        "  plt.savefig(path_grafico_5)\n",
        "  plt.close(fig)\n",
        "  results['grafico_erro_kalman'] = path_grafico_5\n",
        "\n",
        "  #-------------------------------------------------------------------------------------\n",
        "\n",
        "  ### ALTERAÇÃO: Salva o Gráfico 6 (Estado do Filtro de Kalman) ###\n",
        "  fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
        "  fig.suptitle('Estado Estimado pelo Filtro de Kalman')\n",
        "  ax[0].plot(timestamps, state[:, 0])\n",
        "  ax[0].set_ylabel('Ciclos (rad)')\n",
        "  ax[1].plot(timestamps, state[:, 1], label='Cadência (rad/s)')\n",
        "  ax[1].plot(timestamps, state[:, 2], label='$\\phi_0 (rad)')\n",
        "  ax[1].plot(timestamps, state[:, 3], label='$\\phi_1 (rad)')\n",
        "  ax[1].plot(timestamps, state[:, 4], label='$\\phi_2 (rad)')\n",
        "  ax[1].set_ylabel('Fase')\n",
        "  ax[1].set_xlabel('Tempo (s)')\n",
        "  ax[1].legend()\n",
        "  path_grafico_6 = os.path.join(output_dir, f\"{base_filename}_06_estado_kalman.png\")\n",
        "  plt.savefig(path_grafico_6)\n",
        "  plt.close(fig)\n",
        "  results['grafico_estado_kalman'] = path_grafico_6\n",
        "\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Get the timestamps for gait events\n",
        "\n",
        "  timestamps = np.arange(state.shape[0]) / 30.0\n",
        "  get_event_times(state, timestamps)\n",
        "\n",
        "  print(\"\\nAnálise concluída com sucesso!\")\n",
        "  return results\n",
        "\n",
        "# ===============================================================\n",
        "# PARTE 3: LÓGICA DA API E GERENCIAMENTO DE TAREFAS\n",
        "# ===============================================================\n",
        "import uuid\n",
        "import threading\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException, Form\n",
        "from fastapi.responses import FileResponse\n",
        "import asyncio\n",
        "\n",
        "# Armazenamento em memória para o status dos trabalhos\n",
        "# Em um sistema de produção, isso seria um banco de dados como Redis ou similar.\n",
        "# Armazenamento em memória\n",
        "jobs = {}\n",
        "\n",
        "def run_processing(job_id: str, video_path: str, output_path: str, joint_selection: str):\n",
        "    \"\"\"Função que executa o processamento pesado em uma thread separada.\"\"\"\n",
        "    try:\n",
        "        print(f\"--- [Job {job_id}] Iniciando processamento ({joint_selection}) ---\")\n",
        "\n",
        "        # 1. Executa o processamento (UMA VEZ APENAS)\n",
        "        results_paths = processador_de_video(video_path, output_path, joint_selection=joint_selection)\n",
        "\n",
        "        # Verificação de segurança\n",
        "        if not results_paths:\n",
        "            raise Exception(\"O processador retornou vazio (nenhum arquivo gerado).\")\n",
        "\n",
        "        print(f\"--- [Job {job_id}] Processamento finalizado. Preparando lista de arquivos... ---\")\n",
        "\n",
        "        # 2. Extrai apenas os nomes dos arquivos para a resposta da API\n",
        "        # Adicionei uma verificação para garantir que o arquivo existe mesmo\n",
        "        result_files = []\n",
        "        for key, path in results_paths.items():\n",
        "            if path and isinstance(path, str) and os.path.exists(path):\n",
        "                result_files.append(os.path.basename(path))\n",
        "            else:\n",
        "                print(f\"AVISO: Arquivo esperado não encontrado ou nulo: {path}\")\n",
        "\n",
        "        # 3. Atualiza o status\n",
        "        jobs[job_id]['resultados'] = result_files\n",
        "        jobs[job_id]['status'] = 'concluido'\n",
        "\n",
        "        print(f\"--- [Job {job_id}] STATUS DEFINIDO COMO: CONCLUIDO ---\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc() # Imprime o erro completo no log do Colab\n",
        "        print(f\"❌ ERRO no job {job_id}: {e}\")\n",
        "        jobs[job_id]['status'] = 'erro'\n",
        "        jobs[job_id]['error_message'] = str(e)\n",
        "\n",
        "# ===============================================================\n",
        "# PARTE 4: DEFINIÇÃO DOS ENDPOINTS DA API COM FASTAPI\n",
        "# ===============================================================\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/processar\")\n",
        "async def processar_video(file: UploadFile = File(...), joint_selection: str = Form(\"Joelho\")): # <--- Novo parâmetro vindo do Form Data):\n",
        "    \"\"\"Recebe um vídeo, inicia o processamento e retorna um job_id.\"\"\"\n",
        "    job_id = str(uuid.uuid4())\n",
        "\n",
        "    # Cria diretórios para uploads e resultados\n",
        "    upload_dir = \"uploads\"\n",
        "    results_dir = os.path.join(\"resultados\", job_id)\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    video_path = os.path.join(upload_dir, f\"{job_id}_{file.filename}\")\n",
        "\n",
        "    with open(video_path, \"wb\") as buffer:\n",
        "        buffer.write(await file.read())\n",
        "\n",
        "    # Armazena o status inicial do job\n",
        "    jobs[job_id] = {'status': 'processando', 'resultados': None}\n",
        "\n",
        "    # Inicia o processamento em uma thread separada para não bloquear a API\n",
        "    thread = threading.Thread(target=run_processing, args=(job_id, video_path, results_dir, joint_selection))\n",
        "    thread.start()\n",
        "\n",
        "    return {\"message\": \"Processamento iniciado\", \"job_id\": job_id}\n",
        "\n",
        "@app.get(\"/status/{job_id}\")\n",
        "async def get_status(job_id: str):\n",
        "    \"\"\"Verifica o status de um trabalho de processamento.\"\"\"\n",
        "    job = jobs.get(job_id)\n",
        "    if not job:\n",
        "        raise HTTPException(status_code=404, detail=\"Job não encontrado\")\n",
        "    return job\n",
        "\n",
        "@app.get(\"/resultados/{job_id}/{nome_arquivo}\")\n",
        "async def get_resultado(job_id: str, nome_arquivo: str):\n",
        "    \"\"\"Permite o download de um arquivo de resultado.\"\"\"\n",
        "    job = jobs.get(job_id)\n",
        "    if not job or job['status'] != 'concluido':\n",
        "        raise HTTPException(status_code=404, detail=\"Job não concluído ou não encontrado\")\n",
        "\n",
        "    file_path = os.path.join(\"resultados\", job_id, nome_arquivo)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        raise HTTPException(status_code=404, detail=\"Arquivo não encontrado\")\n",
        "\n",
        "    return FileResponse(path=file_path, media_type='application/octet-stream', filename=nome_arquivo)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taLb0QwYgqB8"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# CÉLULA 2 (VERSÃO FINAL E ROBUSTA): Execução da API com URL Estática\n",
        "# ===============================================================\n",
        "import uvicorn\n",
        "from pyngrok import ngrok, conf\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Configuração de Autenticação\n",
        "try:\n",
        "    NGROK_TOKEN = \"33skyLswrrFrO1SFfmXox0W9aWT_7C2hC78WLg9zn1nMx13vj\"\n",
        "    # Opcional: Pegar o domínio dos secrets para não deixar exposto no código\n",
        "    # Se não tiver no secret, você pode escrever a string direto abaixo\n",
        "    NGROK_DOMAIN = \"toucan-glorious-fowl.ngrok-free.app\" # Ex: \"rufino-api.ngrok-free.app\"\n",
        "\n",
        "    if NGROK_TOKEN:\n",
        "        ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    else:\n",
        "        print(\"ERRO CRÍTICO: Secret 'NGROK_AUTHTOKEN' não encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar secrets: {e}\")\n",
        "\n",
        "# 2. Abre o túnel do ngrok com Domínio Estático\n",
        "port = 8000\n",
        "\n",
        "# Fecha túneis anteriores para evitar conflitos ao reexecutar a célula\n",
        "ngrok.kill()\n",
        "\n",
        "try:\n",
        "    if NGROK_DOMAIN:\n",
        "        # Tenta conectar usando o domínio fixo\n",
        "        ssh_tunnel = ngrok.connect(port, domain=NGROK_DOMAIN)\n",
        "        print(f\"✅ API rodando em URL ESTÁTICA: {ssh_tunnel.public_url}\")\n",
        "    else:\n",
        "        # Fallback: Se não tiver domínio configurado, abre um aleatório\n",
        "        print(\"⚠️ NGROK_DOMAIN não definido. Usando URL aleatória.\")\n",
        "        ssh_tunnel = ngrok.connect(port)\n",
        "        print(f\"✅ API rodando em URL ALEATÓRIA: {ssh_tunnel.public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erro ao conectar ngrok: {e}\")\n",
        "    print(\"Dica: Verifique se o domínio está correto no Dashboard do ngrok.\")\n",
        "\n",
        "# 3. Inicia o servidor Uvicorn\n",
        "config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "await server.serve()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}